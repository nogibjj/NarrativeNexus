{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json, os, sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(chunk, client, model=\"gpt-4-turbo\"):\n",
    "    prompt = f\"\"\"\n",
    "I have summaries of 100 news articles and I need to generate multiple-choice questions based on these summaries. \n",
    "The questions should help differentiate between articles and provide a clear understanding of the content just by reading the answers.\n",
    "The questions should be generic to all articles and shouldn't have answer related to one specific article.\n",
    "Make sure to add None of the above as one of the option as I will ask these questions to articles which are not of the same topic. \n",
    "Provide me 20 Questions for the given article summaries. Make sure all the questions are unique and not repeated.\n",
    "Here are the example questions and format I need:\n",
    "\n",
    "1. What is the primary focus of the article?\n",
    "   - A. Military actions and attacks\n",
    "   - B. Political statements and negotiations\n",
    "   - C. Humanitarian impact and casualties\n",
    "   - D. Protests and public reactions\n",
    "   - E. None of the above\n",
    "\n",
    "2. Which international organization is prominently mentioned in the article?\n",
    "   - A. United Nations\n",
    "   - B. NATO\n",
    "   - C. European Union\n",
    "   - D. African Union\n",
    "   - E. None of the above\n",
    "\n",
    "3. What is the main geographical area discussed in the article?\n",
    "   - A. Gaza Strip\n",
    "   - B. West Bank\n",
    "   - C. Israel\n",
    "   - D. International (e.g. U.S. Europe)\n",
    "   - E. None of the above\n",
    "\n",
    "The questions should be comprehensive and cover various aspects like focus, primary actors, geographical area, outcomes, public reaction, and any specific mentions of organizations or events. Please generate similar questions for the given article summaries below:\n",
    "\n",
    "Summaries: {chunk}\n",
    "\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "    except Exception as e:  # if the model fails to return a response\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Sorry, error from GPT.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(input_file, output_file, client, model=\"gpt-4-turbo\"):\n",
    "    # Read the article summaries from the input JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        articles = json.load(f)\n",
    "    \n",
    "    # Extract summaries from the articles\n",
    "    article_summaries = [article['summary'] for article in articles]\n",
    "    num_summaries = len(article_summaries)\n",
    "    batch_size = 50\n",
    "    \n",
    "    responses = []\n",
    "\n",
    "    # Process the article summaries in batches of 100\n",
    "    for i in range(500, num_summaries, batch_size):\n",
    "        batch = article_summaries[i:i+batch_size]\n",
    "        chunk = json.dumps(batch)  # Convert the batch to a JSON string\n",
    "\n",
    "        questions = generate_questions(chunk, client, model=model)\n",
    "        responses.append({\n",
    "            \"batch_start\": i,\n",
    "            \"batch_end\": min(i+batch_size-1, num_summaries-1),\n",
    "            \"questions\": questions\n",
    "        })\n",
    "    \n",
    "    # Write the generated questions to the output JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/QnA_data/combined_summary.json'\n",
    "output_file = '../data/QnA_data/output_questions.json'\n",
    "\n",
    "# Assuming you have your OpenAI API client initialized as `client`\n",
    "process_batches(input_file, output_file, client, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(batch_questions):\n",
    "    questions = {}\n",
    "    q_count = 1\n",
    "    for batch in batch_questions:\n",
    "        batch_qs = batch[\"questions\"].split('\\n\\n')\n",
    "        for q in batch_qs:\n",
    "            match = re.match(r\"\\d+\\. (.+?)\\n\\s+- A\\. (.+?)\\n\\s+- B\\. (.+?)\\n\\s+- C\\. (.+?)\\n\\s+- D\\. (.+?)\\n\\s+- E\\. (.+?)$\", q.strip(), re.DOTALL)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                choices = {\n",
    "                    \"A\": match.group(2).strip(),\n",
    "                    \"B\": match.group(3).strip(),\n",
    "                    \"C\": match.group(4).strip(),\n",
    "                    \"D\": match.group(5).strip(),\n",
    "                    \"E\": match.group(6).strip()\n",
    "                }\n",
    "                if question not in questions:\n",
    "                    questions[question] = choices\n",
    "    return questions\n",
    "\n",
    "def convert_to_json2_format(unique_questions):\n",
    "    formatted_questions = {}\n",
    "    q_num = 1\n",
    "    for question, choices in unique_questions.items():\n",
    "        formatted_questions[f\"Q{q_num}\"] = {\n",
    "            \"question\": question,\n",
    "            \"choices\": choices\n",
    "        }\n",
    "        q_num += 1\n",
    "    return formatted_questions\n",
    "\n",
    "def process_json(input_json):\n",
    "    unique_questions = extract_questions(input_json)\n",
    "    return convert_to_json2_format(unique_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON 1\n",
    "with open('../data/QnA_data/output_questions.json', 'r') as f:\n",
    "    json1 = json.load(f)\n",
    "\n",
    "# Process JSON 1 to get JSON 2 format\n",
    "json2_format = process_json(json1)\n",
    "\n",
    "# Save the result to a new JSON file\n",
    "with open('../data/QnA_data/formated_output_questions.json', 'w') as f:\n",
    "    json.dump(json2_format, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
